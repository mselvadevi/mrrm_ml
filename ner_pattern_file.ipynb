{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67810c46-aa30-49cd-97f4-59509834e9ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded\n",
      "File doesnot exist in file system\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E146] Could not access \\terminilogy.JSON.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m    \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile doesnot exist in file system\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m ruler\u001b[38;5;241m=\u001b[39m\u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_pipe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mentity_ruler\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m verb_list\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVB\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVBN\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVBZ\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVBP\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVBG\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     19\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdd account assignment rule in display page.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\VENV\\NLP\\lib\\site-packages\\spacy\\pipeline\\entityruler.py:513\u001b[0m, in \u001b[0;36mEntityRuler.from_disk\u001b[1;34m(self, path, exclude)\u001b[0m\n\u001b[0;32m    511\u001b[0m     from_disk(path, deserializers_patterns, {})\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# path is not a valid directory or file\u001b[39;00m\n\u001b[1;32m--> 513\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE146\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mpath))\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: [E146] Could not access \\terminilogy.JSON."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import os\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "print(\"Models loaded\")\n",
    "#ruler = nlp.add_pipe(\"entity_ruler\",after='ner')\n",
    "\"\"\" pipeline to detect technical terms - Load  patterns from JSON file\n",
    "STATUS:UNDER DEVELOPMENT\n",
    "\"\"\"\n",
    "text_file_path= r\"\\terminilogy.JSON\"\n",
    "if(os.path.exists(text_file_path)):\n",
    "   print(\"file path:{0}\".format(text_file_path))\n",
    "else:\n",
    "   print(\"File doesnot exist in file system\")\n",
    "\n",
    "ruler=nlp.add_pipe(\"entity_ruler\").from_disk(text_file_path)\n",
    "\n",
    "verb_list=['VB','VBN','VBZ','VBP','VBG']\n",
    "\n",
    "doc = nlp(\"Add account assignment rule in display page.\")\n",
    "doc = nlp(\"Add account assignment rule in display page. account team details shall be displayed on this page\")\n",
    "doc=nlp(\"Add account assignment rule in display page. account team details shall be displayed on this page. yeah well um. i've been given a few instructions as how they would like to see a cosine working from salesforce so i'm i'm currently documenting all of those requests. Okay, and it may involve being able to pick templates from a drop down menu, and you know, having everything available from the from the access that we've got within salesforce.\".lower())\n",
    "spacy.displacy.render(doc.sents,style='dep')\n",
    "for sentence in doc.sents:\n",
    "    for chunk in sentence.noun_chunks:\n",
    "        pass#print(chunk)\n",
    "print(\"ents\")\n",
    "for sentence in doc.sents:\n",
    "\n",
    "    for ent in sentence.ents:\n",
    "        child=ent.root.head\n",
    "        print(child.text)\n",
    "        #child.dep_ == 'dobj' and child.pos_ == 'NOUN' and child.head.pos_==\"VERB\" and  child.head.tag_ in verb_list\n",
    "        print (child.subtree.text, ent.label_,\"\\t#\",ent.root.head, \"\\t$\", \"\\t#\",child.head.pos_, \"\\t@\", child.head.tag_)\n",
    "        if (ent.label_ == \"TECH_TERM\" and child.head.pos_==\"VERB\" and  child.head.tag_ in verb_list):\n",
    "            print(\"requirement :\",sentence)\n",
    "            patt=\" \"+ child.head.text+ \" @ \" + ' '.join([token.text for token in child.subtree])\n",
    "            print(\"Pattern is:\",patt)\n",
    "        else:\n",
    "            print(\"Not:\",sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2ca9be9-f659-4ac7-b412-b923e30e3e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern file path:E:\\PRELUDESYS\\terminilogy.JSONL\n",
      "requirement : add account assignment rule in display page.\n",
      "requirement : account team details shall be displayed on this page.\n",
      "requirement : i've been given a few instructions as how they would like to see a cosine working from salesforce so i'm i'm currently documenting all of those requests.\n",
      "requirement : okay, and it may involve being able to pick templates from a drop down menu, and you know, having everything available from the from the access that we've got within salesforce.\n",
      "[' add @ add account assignment rule in display page .', ' displayed @ account team details', ' working @ from salesforce', ' got @ within salesforce']\n"
     ]
    }
   ],
   "source": [
    "tech_term_patterns=[]\n",
    "def lookup_for_tech_terms(my_conversation,text_file_path):\n",
    "    global tech_term_patterns\n",
    "    if(os.path.exists(text_file_path)):\n",
    "        print(\"Pattern file path:{0}\".format(text_file_path))\n",
    "        \n",
    "    else:\n",
    "        print(\"File does not exist in file system\")\n",
    "        return tech_term_patterns\n",
    "    \n",
    "    if(\"entity_ruler\" not in nlp.pipe_names):\n",
    "        ruler=nlp.add_pipe(\"entity_ruler\").from_disk(text_file_path)\n",
    "    doc=nlp(my_conversation.lower())\n",
    "    for sentence in doc.sents:\n",
    "        for ent in sentence.ents:\n",
    "            child=ent.root.head\n",
    "            #print(child.text)\n",
    "            #child.dep_ == 'dobj' and child.pos_ == 'NOUN' and child.head.pos_==\"VERB\" and  child.head.tag_ in verb_list\n",
    "            #print (ent.text, ent.label_,\"\\t#\",ent.root.head, \"\\t$\", \"\\t#\",child.head.pos_, \"\\t@\", child.head.tag_)\n",
    "            if (ent.label_ == \"TECH_TERM\" and child.head.pos_==\"VERB\" and  child.head.tag_ in verb_list):\n",
    "                print(\"requirement :\",sentence)\n",
    "                req_tech_pattern=\" \"+ child.head.text+ \" @ \" + ' '.join([token.text for token in child.subtree])\n",
    "                #print(\"Pattern is:\",patt)\n",
    "                tech_term_patterns.append(req_tech_pattern)\n",
    "            else:\n",
    "                pass\n",
    "                #print(\"Not:\",sentence)\n",
    "\n",
    "    return tech_term_patterns\n",
    "        \n",
    "my_converstaion=\"Add account assignment rule in display page. account team details shall be displayed on this page. yeah well um. i've been given a few instructions as how they would like to see a cosine working from salesforce so i'm i'm currently documenting all of those requests. Okay, and it may involve being able to pick templates from a drop down menu, and you know, having everything available from the from the access that we've got within salesforce.\"   \n",
    "ent_req_tech_term=lookup_for_tech_terms(my_converstaion,text_file_path)\n",
    "print(ent_req_tech_term)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e773d66f-3d9b-4c55-8a39-a93d808fea44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OS\n",
      " Volume Serial Number is 1A39-B7AA\n",
      "\n",
      " Directory of C:\\Users\\murugesan_r\\OneDrive - Prelude Systems Inc\\PRELUDESYS\\GITHUB\\KPO\n",
      "\n",
      "05/22/2023  03:33 PM    <DIR>          .\n",
      "05/22/2023  03:33 PM    <DIR>          ..\n",
      "05/22/2023  03:27 PM    <DIR>          .ipynb_checkpoints\n",
      "04/13/2023  08:42 PM         2,458,697 21-123-1930.PDF\n",
      "03/31/2023  09:59 AM           143,430 3.pdf\n",
      "05/22/2023  03:31 PM            30,680 auto_nn.ipynb\n",
      "05/08/2023  12:25 PM    <DIR>          bert-base-uncased\n",
      "05/12/2023  04:25 PM    <DIR>          DATA-SETS\n",
      "05/08/2023  10:11 AM    <DIR>          datasets\n",
      "05/12/2023  09:44 AM             2,663 data_process_util.py\n",
      "05/11/2023  12:02 PM           524,288 kpo.db\n",
      "05/16/2023  06:23 PM            28,053 kpo.ipynb\n",
      "05/10/2023  10:38 AM           149,657 KPO_generalized_BERT_Pre_TRAIN.ipynb\n",
      "04/27/2023  01:48 PM            46,117 kpo_moder_serve.ipynb\n",
      "04/27/2023  01:45 PM            30,996 kpo_moder_serve.py\n",
      "05/19/2023  06:28 PM            10,780 ListOfCategories.xlsx\n",
      "05/19/2023  11:10 AM           126,929 log_train.txt\n",
      "03/20/2023  10:34 AM            38,781 Methodology-Architecture.docx\n",
      "05/19/2023  11:37 AM            11,860 methodology.xlsx\n",
      "05/11/2023  12:02 PM    <DIR>          mlartifacts\n",
      "05/11/2023  06:18 PM    <DIR>          mlruns\n",
      "04/27/2023  01:31 PM    <DIR>          MODELS\n",
      "03/31/2023  03:04 PM               920 NER_LIST.txt\n",
      "05/22/2023  03:33 PM            10,725 ner_pattern_file.ipynb\n",
      "05/19/2023  02:57 PM             4,854 NER_tagger.ipynb\n",
      "05/19/2023  02:57 PM            40,380 NER_tagger_10_05_22.ipynb\n",
      "05/19/2023  02:57 PM           220,965 NER_tagger_16_05_22.ipynb\n",
      "05/11/2023  05:43 PM    <DIR>          output\n",
      "03/31/2023  11:45 AM               469 pdf_reader.py\n",
      "05/12/2023  09:44 AM            12,947 pdf_to_text.ipynb\n",
      "04/13/2023  08:44 PM            20,179 pdf_to_text.ipynb.orig\n",
      "10/01/2018  09:19 AM    <DIR>          poppler-0.68.0\n",
      "03/31/2023  11:58 AM         7,296,506 poppler-0.68.0_x86.7z\n",
      "04/17/2023  06:59 PM            38,810 presentation.pptx\n",
      "05/19/2023  06:28 PM            10,098 Prototype_Approach results.xlsx\n",
      "05/22/2023  02:00 PM           824,230 provider.jsonl\n",
      "05/22/2023  02:53 PM         1,521,663 ProviderList.xlsx\n",
      "05/12/2023  05:25 PM             4,308 read_utils.py\n",
      "04/03/2023  01:37 PM            10,763 spark_nlp.ipynb\n",
      "03/31/2023  01:56 PM           885,197 SRS_AIML_HighlevelRequirement_V1.0.docx\n",
      "02/17/2023  10:06 AM                11 start_jupyter.bat\n",
      "04/13/2023  08:45 PM                28 start_jupyter.sh\n",
      "04/13/2023  08:45 PM                28 start_jupyter.sh.orig\n",
      "05/10/2023  10:38 AM               274 test.py\n",
      "05/12/2023  09:44 AM                 0 text_to_json.py\n",
      "03/31/2023  11:34 AM        31,971,558 thonny-4.0.2-windows-portable.zip\n",
      "05/12/2023  09:44 AM             1,722 torch_metrics.py\n",
      "05/14/2023  09:37 PM             8,279 train.py\n",
      "05/12/2023  11:44 AM               617 Untitled.ipynb\n",
      "05/08/2023  11:25 AM                 0 untitled.py\n",
      "04/07/2023  09:21 AM                 0 untitled.r\n",
      "05/22/2023  02:02 PM                72 Untitled1.ipynb\n",
      "05/22/2023  02:38 PM                72 Untitled2.ipynb\n",
      "05/22/2023  03:06 PM                72 Untitled3.ipynb\n",
      "04/19/2023  11:22 AM               101 wsgi.py\n",
      "              44 File(s)     46,488,779 bytes\n",
      "              11 Dir(s)  14,708,518,912 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7bfe920-340a-4068-8653-48287e4b180f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "welcome murugesan hi\n"
     ]
    }
   ],
   "source": [
    "my_list= ['welcome','murugesan', 'hi']\n",
    "print(' '.join(my_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ca828-0b34-4dea-81a4-593add7a5c46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9fcfa40-790a-4b8f-a3c8-922ca1ef85c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"label\": \"TECH_TERM\", \"pattern\":\"Account\"}\n"
     ]
    }
   ],
   "source": [
    "line=\"Account\"\n",
    "pattern = '{\"label\": \"TECH_TERM\", \"pattern\":'+ '\"'+ line.rstrip() +'\"}'\n",
    "print(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc614f75-a8d5-4058-8d1d-5ff3a1bc6c71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265f3a1c-a40a-4c5b-99e4-2e43badeaf03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
